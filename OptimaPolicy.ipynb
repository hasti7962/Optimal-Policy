{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKXGaEP4BbYq",
        "outputId": "9e41461e-2931-44f7-814f-904a53c9a66f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy: [3 3 0 3 3 0 3 0 0]\n",
            "Average Reward over 100 episodes: 9.787799999999999\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class GridEnvironment:\n",
        "    def __init__(self, gamma=0.9):\n",
        "        self.grid_size = (3, 3)  # 3x3 grid\n",
        "        self.states = [(i, j) for i in range(self.grid_size[0]) for j in range(self.grid_size[1])]\n",
        "        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
        "\n",
        "        # Initialize rewards\n",
        "        self.rewards = {(0, 2): 10, (2, 0): -10}  # Specific rewards for terminal states\n",
        "        for x in range(self.grid_size[0]):\n",
        "            for y in range(self.grid_size[1]):\n",
        "                if (x, y) not in [(0, 2), (2, 0)]:\n",
        "                    self.rewards[(x, y)] = -0.01  # Life reward\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.value_table = np.zeros(len(self.states))\n",
        "        self.policy = np.zeros(len(self.states), dtype=int)\n",
        "\n",
        "    def get_state_index(self, state):\n",
        "        return state[0] * self.grid_size[1] + state[1]\n",
        "\n",
        "    def get_next_state(self, state, action):\n",
        "        prob_success = 0.8\n",
        "        prob_left = 0.1\n",
        "        prob_right = 0.1\n",
        "\n",
        "        if action == 'UP':\n",
        "            next_states = [(max(state[0] - 1, 0), state[1]), (state[0], state[1]), (min(state[0] + 1, 2), state[1])]\n",
        "        elif action == 'DOWN':\n",
        "            next_states = [(min(state[0] + 1, 2), state[1]), (state[0], state[1]), (max(state[0] - 1, 0), state[1])]\n",
        "        elif action == 'LEFT':\n",
        "            next_states = [(state[0], max(state[1] - 1, 0)), (state[0], state[1]), (state[0], min(state[1] + 1, 2))]\n",
        "        elif action == 'RIGHT':\n",
        "            next_states = [(state[0], min(state[1] + 1, 2)), (state[0], state[1]), (state[0], max(state[1] - 1, 0))]\n",
        "\n",
        "        probabilities = [prob_success, prob_left / 2, prob_right / 2]\n",
        "        next_state_probs = {state: 0 for state in self.states}\n",
        "\n",
        "        for next_state, prob in zip(next_states, probabilities):\n",
        "            next_state_probs[next_state] += prob\n",
        "\n",
        "        return next_state_probs\n",
        "\n",
        "    def value_iteration(self, theta=1e-6):\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for state in self.states:\n",
        "                v = self.value_table[self.get_state_index(state)]\n",
        "                value_action = []\n",
        "                for action in self.actions:\n",
        "                    next_state_probs = self.get_next_state(state, action)\n",
        "                    value = sum(prob * (self.rewards.get(next_state, 0) + self.gamma * self.value_table[self.get_state_index(next_state)])\n",
        "                                for next_state, prob in next_state_probs.items())\n",
        "                    value_action.append(value)\n",
        "                self.value_table[self.get_state_index(state)] = max(value_action)\n",
        "                delta = max(delta, abs(v - self.value_table[self.get_state_index(state)]))\n",
        "            if delta < theta:\n",
        "                break\n",
        "\n",
        "    def extract_policy(self):\n",
        "        for state in self.states:\n",
        "            q_values = []\n",
        "            for action in self.actions:\n",
        "                next_state_probs = self.get_next_state(state, action)\n",
        "                q_value = sum(prob * (self.rewards.get(next_state, 0) + self.gamma * self.value_table[self.get_state_index(next_state)])\n",
        "                              for next_state, prob in next_state_probs.items())\n",
        "                q_values.append(q_value)\n",
        "            self.policy[self.get_state_index(state)] = np.argmax(q_values)\n",
        "\n",
        "    def simulate(self, episodes=100):\n",
        "        total_rewards = []\n",
        "        for _ in range(episodes):\n",
        "            state = random.choice(self.states)  # Random starting state\n",
        "            total_reward = 0\n",
        "            while True:\n",
        "                action = self.actions[self.policy[self.get_state_index(state)]]\n",
        "                next_state_probs = self.get_next_state(state, action)\n",
        "                next_states = list(next_state_probs.keys())\n",
        "                probabilities = list(next_state_probs.values())\n",
        "                state = random.choices(next_states, probabilities)[0]\n",
        "                total_reward += self.rewards.get(state, 0)\n",
        "                if state in [(0, 2), (2, 0)]:  # Terminal states\n",
        "                    break\n",
        "            total_rewards.append(total_reward)\n",
        "        return np.mean(total_rewards)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    gamma_value = 0.9\n",
        "    environment = GridEnvironment(gamma=gamma_value)\n",
        "\n",
        "    # Perform Value Iteration\n",
        "    environment.value_iteration()\n",
        "\n",
        "    # Extract the optimal policy\n",
        "    environment.extract_policy()\n",
        "\n",
        "    # Simulate episodes and evaluate average reward\n",
        "    average_reward = environment.simulate(episodes=100)\n",
        "\n",
        "    print(\"Optimal Policy:\", environment.policy)\n",
        "    print(\"Average Reward over 100 episodes:\", average_reward)"
      ]
    }
  ]
}